# russia_thinktank_bot.py
import os
import re
import time
import logging
import requests
from bs4 import BeautifulSoup
from deep_translator import GoogleTranslator
import schedule
from dotenv import load_dotenv

load_dotenv()

TELEGRAM_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID", "@time_n_John")

if not TELEGRAM_TOKEN:
    raise ValueError("‚ùå TELEGRAM_BOT_TOKEN –Ω–µ –∑–∞–¥–∞–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")

# ================== –ò–°–¢–û–ß–ù–ò–ö–ò ==================
SOURCES = [
    {"name": "Foreign Affairs", "url": "https://www.foreignaffairs.com/rss.xml"},
    {"name": "Reuters Institute", "url": "https://reutersinstitute.politics.ox.ac.uk/rss.xml"},
    {"name": "Bruegel", "url": "https://www.bruegel.org/rss.xml"},
    {"name": "E3G", "url": "https://www.e3g.org/feed/"},
    {"name": "Chatham House", "url": "https://www.chathamhouse.org/rss.xml"},
    {"name": "CSIS", "url": "https://www.csis.org/rss.xml"},
    {"name": "Atlantic Council", "url": "https://www.atlanticcouncil.org/feed/"},
    {"name": "RAND Corporation", "url": "https://www.rand.org/rss.xml"},
    {"name": "CFR", "url": "https://www.cfr.org/rss/"},
    {"name": "Carnegie Endowment", "url": "https://carnegieendowment.org/rss.xml"},
    {"name": "The Economist", "url": "https://www.economist.com/latest/rss.xml"},
    {"name": "Bloomberg Politics", "url": "https://www.bloomberg.com/politics/feeds/site.xml"},
]

KEYWORDS = [
    r"\brussia\b", r"\brussian\b", r"\bputin\b", r"\bmoscow\b", r"\bkremlin\b",
    r"\bukraine\b", r"\bukrainian\b", r"\bzelensky\b", r"\bkyiv\b", r"\bkiev\b",
    r"\bcrimea\b", r"\bdonbas\b", r"\bsanction[s]?\b", r"\bgazprom\b",
    r"\bnord\s?stream\b", r"\bwagner\b", r"\blavrov\b", r"\bshoigu\b",
    r"\bmedvedev\b", r"\bpeskov\b", r"\bnato\b", r"\beuropa\b", r"\busa\b",
    r"\bwar\b", r"\bconflict\b", r"\bmilitary\b", r"\bruble\b", r"\beconomy\b",
    r"\benergy\b", r"\boil\b", r"\bgas\b", r"\bsoviet\b", r"\bpost\W?soviet\b"
]

MAX_SEEN = 5000
MAX_PER_RUN = 12
seen_links = set()

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
log = logging.getLogger(__name__)

def clean_text(t):
    return re.sub(r"\s+", " ", t).strip()

def translate_to_russian(text):
    try:
        return GoogleTranslator(source='auto', target='ru').translate(text)
    except Exception as e:
        log.warning(f"‚ö†Ô∏è –ü–µ—Ä–µ–≤–æ–¥ –Ω–µ —É–¥–∞–ª—Å—è: {e}")
        return text

def get_summary(title):
    low = title.lower()
    if re.search(r"sanction|embargo|restrict", low):
        return "–í–≤–µ–¥–µ–Ω—ã –Ω–æ–≤—ã–µ —Å–∞–Ω–∫—Ü–∏–∏ –∏–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è."
    if re.search(r"war|attack|strike|bomb|conflict|military", low):
        return "–°–æ–æ–±—â–∞–µ—Ç—Å—è –æ –≤–æ–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏—è—Ö –∏–ª–∏ —É–¥–∞—Ä–∞—Ö."
    if re.search(r"putin|kremlin|peskov|moscow", low):
        return "–ó–∞—è–≤–ª–µ–Ω–∏–µ –∏–ª–∏ –¥–µ–π—Å—Ç–≤–∏–µ —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã –ö—Ä–µ–º–ª—è."
    if re.search(r"economy|rubl?e|oil|gas|gazprom|nord\s?stream|energy", low):
        return "–ù–æ–≤–æ—Å—Ç–∏ —ç–∫–æ–Ω–æ–º–∏–∫–∏, –Ω–µ—Ñ—Ç–∏, –≥–∞–∑–∞ –∏–ª–∏ —Ä—É–±–ª—è."
    if re.search(r"diplomat|talks|negotiat|meeting|lavrov", low):
        return "–î–∏–ø–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã –∏–ª–∏ –∫–æ–Ω—Ç–∞–∫—Ç—ã."
    if re.search(r"wagner|shoigu|medvedev|defense", low):
        return "–°–æ–±—ã—Ç–∏—è —Å —Ä–æ—Å—Å–∏–π—Å–∫–∏–º–∏ –≤–æ–µ–Ω–Ω—ã–º–∏ –∏–ª–∏ –ø–æ–ª–∏—Ç–∏–∫–∞–º–∏."
    if re.search(r"ukraine|zelensky|kyiv|kiev|crimea|donbas", low):
        return "–°–æ–±—ã—Ç–∏—è, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –£–∫—Ä–∞–∏–Ω–æ–π –∏ –ø—Ä–∏–ª–µ–≥–∞—é—â–∏–º–∏ —Ä–µ–≥–∏–æ–Ω–∞–º–∏."
    if re.search(r"nato|europa|european|germany|france|usa|uk", low):
        return "–†–µ–∞–∫—Ü–∏—è –∑–∞–ø–∞–¥–Ω—ã—Ö —Å—Ç—Ä–∞–Ω –∏–ª–∏ –ù–ê–¢–û –Ω–∞ —Å–æ–±—ã—Ç–∏—è —Å —É—á–∞—Å—Ç–∏–µ–º –†–æ—Å—Å–∏–∏."
    return "–ê–Ω–∞–ª–∏—Ç–∏–∫–∞, —Å–≤—è–∑–∞–Ω–Ω–∞—è —Å –†–æ—Å—Å–∏–µ–π –∏–ª–∏ –ø–æ—Å—Ç—Å–æ–≤–µ—Ç—Å–∫–∏–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ–º."

def get_source_prefix(name):
    name = name.lower()
    if "e3g" in name:
        return "e3g"
    elif "foreign affairs" in name:
        return "foreignaffairs"
    elif "chatham house" in name:
        return "chathamhouse"
    elif "csis" in name:
        return "csis"
    elif "atlantic council" in name:
        return "atlanticcouncil"
    elif "rand" in name:
        return "rand"
    elif "cfr" in name:
        return "cfr"
    elif "carnegie" in name:
        return "carnegie"
    elif "bruegel" in name:
        return "bruegel"
    elif "bloomberg" in name:
        return "bloomberg"
    elif "reuters institute" in name:
        return "reuters"
    elif "the economist" in name:
        return "economist"
    else:
        return name.split()[0].lower()

def fetch_rss_news():
    global seen_links
    result = []
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

    for src in SOURCES:
        if len(result) >= MAX_PER_RUN:
            break
        try:
            url = src["url"].strip()
            log.info(f"üì° {src['name']}")
            resp = requests.get(url, timeout=30, headers=headers)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.content, "xml")

            for item in soup.find_all("item"):
                if len(result) >= MAX_PER_RUN:
                    break

                title = clean_text(item.title.get_text()) if item.title else ""
                link = (item.link.get_text() or item.guid.get_text()).strip() if item.link or item.guid else ""

                if not title or not link or link in seen_links:
                    continue

                if not any(re.search(kw, title, re.IGNORECASE) for kw in KEYWORDS):
                    continue

                # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ RSS
                description = ""
                desc_tag = item.find("description") or item.find("content:encoded")
                if desc_tag:
                    desc_html = desc_tag.get_text()
                    desc_soup = BeautifulSoup(desc_html, "html.parser")
                    raw_desc = clean_text(desc_soup.get_text())
                    # –û–±—Ä–µ–∑–∞–µ–º –¥–æ 300‚Äì400 —Å–∏–º–≤–æ–ª–æ–≤, –Ω–æ –Ω–µ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ —Å–ª–æ–≤–∞
                    if len(raw_desc) > 400:
                        description = raw_desc[:400].rsplit(' ', 1)[0] + "‚Ä¶"
                    else:
                        description = raw_desc

                # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏—è –Ω–µ—Ç ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑—é–º–µ
                if not description.strip():
                    description = get_summary(title)

                ru_title = translate_to_russian(title)
                prefix = get_source_prefix(src["name"])
                msg = f"{prefix}: {ru_title}\n\n{description}\n\n–ò—Å—Ç–æ—á–Ω–∏–∫ ({link})"
                result.append({"msg": msg, "link": link})

        except Exception as e:
            log.error(f"‚ùå {src['name']}: {e}")

    return result

def send_to_telegram(text):
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    payload = {
        "chat_id": CHANNEL_ID,
        "text": text,
        "parse_mode": "Markdown",  # –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å, –Ω–æ —Ç–µ–∫—Å—Ç —á–∏—Å—Ç—ã–π
        "disable_web_page_preview": True,
    }
    try:
        r = requests.post(url, data=payload, timeout=15)
        if r.status_code == 200:
            log.info("‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ")
        else:
            log.error(f"‚ùå Telegram error: {r.text}")
    except Exception as e:
        log.error(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")

def job():
    global seen_links
    log.info("üîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –†–æ—Å—Å–∏–∏ –∏ –≥–µ–æ–ø–æ–ª–∏—Ç–∏–∫–µ...")
    news = fetch_rss_news()
    if not news:
        log.info("üì≠ –ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π.")
        return

    for item in news:
        send_to_telegram(item["msg"])
        seen_links.add(item["link"])
        if len(seen_links) > MAX_SEEN:
            seen_links = set(list(seen_links)[-4000:])
        time.sleep(1)

if __name__ == "__main__":
    log.info("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω –Ω–∞ Render. –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: %d", len(SOURCES))
    job()
    schedule.every(30).minutes.do(job)

    while True:
        schedule.run_pending()
        time.sleep(1)